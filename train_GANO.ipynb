{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d040166a",
   "metadata": {},
   "source": [
    "# Train the GANO model using kik-net data\n",
    "\n",
    "for detail of the model implementation, please refer to [Shi et al, 2023](https://arxiv.org/abs/2309.03447) and [Rahman et al, 2022](https://arxiv.org/abs/2205.03017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb3c12d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import pickle as pkl \n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, './Python_libs')\n",
    "# load GP function\n",
    "from random_fields import *\n",
    "\n",
    "# load GANO model\n",
    "from imp import reload\n",
    "import GANO_model\n",
    "reload(GANO_model)\n",
    "from GANO_model import Generator, Discriminator\n",
    "\n",
    "# import utils\n",
    "from dataUtils_3C import SeisData\n",
    "import os\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "546f3ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# adjust the layout of jupyternoteook\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04317a5a",
   "metadata": {},
   "source": [
    "## Training parameters of GANO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "029701b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GANO model parameters\n",
    "ndim = 6000      # dimension of 1D time history\n",
    "npad = 400       # pad at the end, to guarantee the length of data is the power of 2 (efficient FFT)\n",
    "width= 32        # lift the dimension of input from 3 -> width\n",
    "lr = 1e-4        # learning rate\n",
    "\n",
    "# single GPU training\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Training parameters\n",
    "epochs = 30      # training epochs\n",
    "λ_grad = 10.0    # penatly factor\n",
    "n_critic = 10    # train D n_critic times before train G\n",
    "batch_size = 64  # decrease batch_size if cuda is out of memory\n",
    "\n",
    "grf = GaussianRF_idct(1,(ndim + npad), alpha=1.5, tau=1.0, cal1d=True, device=device) # 1D GRF (GP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f888ee2",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "\n",
    "In order to use the dataloader, please prepare your dataset. Following files are needed \n",
    "1. `Full dataset` : N records with three-component waveforms and the same sampling frequency, each record has a length of `ndim` with the onset of P wave. (datasets of velocity time histories and corresponding acceleration time histories are both needed)\n",
    "\n",
    "\n",
    "2. `attribute file`: should contain the meta information associated with the records, corresponding to `condv_names` variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e415fda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "Loaded samples:  42481\n",
      "normalizing data ...\n",
      "max log pga: 0.09604963680854223 min log pga: -4.779544184431937\n",
      "--------- magnitude -----------\n",
      "min magnitude 4.5 scale min 4.5\n",
      "max magnitude 8.0 scale max 8.0\n",
      "vc shape (42481, 1)\n",
      "--------- rrup -----------\n",
      "min rrup 3.190180152 scale min 0\n",
      "max rrup 299.9925061 scale max 300\n",
      "vc shape (42481, 1)\n",
      "--------- vs30 -----------\n",
      "min vs30 111.11411939894413 scale min 100\n",
      "max vs30 1097.5609756097565 scale max 1100\n",
      "vc shape (42481, 1)\n",
      "--------- tectonic_value -----------\n",
      "min tectonic_value 0.0 scale min 0\n",
      "max tectonic_value 1.0 scale max 1\n",
      "vc shape (42481, 1)\n",
      "Number selected samples:  38232\n",
      "Class init done!\n",
      "total Train: 38232\n",
      "Loading data ...\n",
      "Loaded samples:  42481\n",
      "normalizing data ...\n",
      "max log pga: 0.09604963680854223 min log pga: -4.779544184431937\n",
      "--------- magnitude -----------\n",
      "min magnitude 4.5 scale min 4.5\n",
      "max magnitude 8.0 scale max 8.0\n",
      "vc shape (42481, 1)\n",
      "--------- rrup -----------\n",
      "min rrup 3.190180152 scale min 0\n",
      "max rrup 299.9925061 scale max 300\n",
      "vc shape (42481, 1)\n",
      "--------- vs30 -----------\n",
      "min vs30 111.11411939894413 scale min 100\n",
      "max vs30 1097.5609756097565 scale max 1100\n",
      "vc shape (42481, 1)\n",
      "--------- tectonic_value -----------\n",
      "min tectonic_value 0.0 scale min 0\n",
      "max tectonic_value 1.0 scale max 1\n",
      "vc shape (42481, 1)\n",
      "Number selected samples:  4249\n",
      "Class init done!\n",
      "total Validation: 4249\n",
      "shape wfs: (2, 3, 6000)\n",
      "shape log10_PGA: (2, 3)\n"
     ]
    }
   ],
   "source": [
    "config_d = {\n",
    "    \n",
    "    'data_file': './kik_net_data/vel_60s_final.npy',       # full dataset, shape [N, 3, ndim]\n",
    "    'attr_file': './kik_net_data/attrs_60s_final.csv',     # attribute file, contains magnitude, rupture distance, vs30, etc... for each record. \n",
    "    'batch_size': batch_size,\n",
    "\n",
    "    'frac_train': 0.9,                                              # fraction of training\n",
    "    'condv_names': ['magnitude','rrup', 'vs30', 'tectonic_value'],  # name of conditional variables\n",
    "    'condv_min_max' : [(4.5, 8.0), (0, 300), (100, 1100), (0,1)]    # [min, max] for each conditional variable\n",
    "\n",
    "}\n",
    "\n",
    "# load the train and val indexes, guarantee reproductivity\n",
    "index = np.load('./kik_net_data/index_100Hz_final.npy', allow_pickle=True)\n",
    "ix_train = index[0]                                                 # index of training dataset                         \n",
    "ix_val = index[1]                                                   # index of validation dataset\n",
    "\n",
    "\"\"\"\n",
    "# run this part if you don't have the index file\n",
    "# Shuffle the data and save the index.\n",
    "\n",
    "Ntot = len(pd.read_csv(config_d['attr_file']))\n",
    "\n",
    "frac = config_d['frac_train']\n",
    "Nbatch = config_d['batch_size']\n",
    "\n",
    "# get all indexes\n",
    "ix_all = np.arange(Ntot)\n",
    "# get training indexes\n",
    "Nsel = int(Ntot*frac)\n",
    "\n",
    "ix_train = np.random.choice(ix_all, size=Nsel, replace=False)\n",
    "ix_train.sort()\n",
    "# get validation indexes\n",
    "ix_val = np.setdiff1d(ix_all, ix_train, assume_unique=True)\n",
    "ix_val.sort()\n",
    "\n",
    "index = []\n",
    "index.append(ix_train)\n",
    "index.append(ix_val)\n",
    "np.save('./kik_net_data/index_100Hz_final.npy', index)\n",
    "\"\"\"\n",
    "\n",
    "# data loader\n",
    "sdat_train = SeisData(config_d['data_file'], config_d['attr_file'],config_d['condv_names'], config_d['condv_min_max'], batch_size=config_d['batch_size'], isel=ix_train)\n",
    "print('total Train:', sdat_train.get_Ntrain())\n",
    "\n",
    "sdat_val = SeisData(config_d['data_file'], config_d['attr_file'],config_d['condv_names'], config_d['condv_min_max'], batch_size= config_d['batch_size'], isel=ix_val)\n",
    "\n",
    "n_train_tot = sdat_train.get_Nbatches_tot()\n",
    "\n",
    "print('total Validation:', sdat_val.get_Ntrain())\n",
    "# get random samples [batch, 3, dimension], normalized log10_PGA [batch, 3], conditonal variables[[batch], ..., [batch]] \n",
    "(wfs, log10_PGA, cvs) = sdat_val.get_rand_batch()\n",
    "print('shape wfs:', wfs.shape)\n",
    "print('shape log10_PGA:', log10_PGA.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a451aa0",
   "metadata": {},
   "source": [
    "## Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a97787e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number discriminator parameters:  136627873\n",
      "Number generator parameters:  136628358\n"
     ]
    }
   ],
   "source": [
    "D = Discriminator(6+4, width, ndim=ndim,pad=npad).to(device)    # 6 (3 waveforms+3 PGAs) + 4 (4 conditional variables)\n",
    "G = Generator(1+4, width, ndim=ndim, pad=npad, training=True).to(device)       # 1 (GP) + 4 (4 conditional variables)\n",
    "\n",
    "#D = DDP(D, device_ids=device_ids, output_device=0) #data parallel, multi-gpu training\n",
    "#G = DDP(D, device_ids=device_ids, output_device=0)\n",
    "\n",
    "nn_params = sum(p.numel() for p in D.parameters() if p.requires_grad)\n",
    "print(\"Number discriminator parameters: \", nn_params)\n",
    "nn_params = sum(p.numel() for p in G.parameters() if p.requires_grad)\n",
    "print(\"Number generator parameters: \", nn_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2651958c",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34929a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (fc0): Linear(in_features=5, out_features=32, bias=True)\n",
       "  (conv0): SpectralConv1d()\n",
       "  (conv1): SpectralConv1d()\n",
       "  (conv2): SpectralConv1d()\n",
       "  (conv2_1): SpectralConv1d()\n",
       "  (conv2_9): SpectralConv1d()\n",
       "  (conv3): SpectralConv1d()\n",
       "  (conv4): SpectralConv1d()\n",
       "  (conv5): SpectralConv1d()\n",
       "  (w0): pointwise_op(\n",
       "    (conv): Conv1d(32, 48, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (w1): pointwise_op(\n",
       "    (conv): Conv1d(48, 96, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (w2): pointwise_op(\n",
       "    (conv): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (w2_1): pointwise_op(\n",
       "    (conv): Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (w2_9): pointwise_op(\n",
       "    (conv): Conv1d(384, 192, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (w3): pointwise_op(\n",
       "    (conv): Conv1d(384, 96, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (w4): pointwise_op(\n",
       "    (conv): Conv1d(192, 48, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (w5): pointwise_op(\n",
       "    (conv): Conv1d(96, 32, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_optim = torch.optim.Adam(G.parameters(), lr=lr , weight_decay=1e-4)               # optimizer\n",
    "D_optim = torch.optim.Adam(D.parameters(), lr=lr , weight_decay=1e-4)\n",
    "G_scheduler = torch.optim.lr_scheduler.StepLR(G_optim, step_size=5, gamma=0.8)      # step learnig rate\n",
    "D_scheduler = torch.optim.lr_scheduler.StepLR(D_optim, step_size=5, gamma=0.8)\n",
    "\n",
    "D.train()\n",
    "G.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a608e10",
   "metadata": {},
   "source": [
    "## Graident penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e66097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient_penalty(model, real_images, fake_images, label,device):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "\n",
    "    alpha = torch.randn((real_images.size(0), 1, 1), device=device)\n",
    "    interpolates_wfs = (alpha * real_images + ((1 - alpha) * fake_images)).requires_grad_(True)\n",
    "    \n",
    "    #print(interpolates_wfs.shape, interpolates_lcn.shape)\n",
    "    model_interpolates = model(interpolates_wfs, label)\n",
    "    grad_outputs = torch.ones(model_interpolates.size(), device=device, requires_grad=False)\n",
    "\n",
    "    # Get gradient w.r.t. interpolates\n",
    "    grad_wf = torch.autograd.grad(\n",
    "        outputs=model_interpolates,\n",
    "        inputs=interpolates_wfs,\n",
    "        grad_outputs=grad_outputs,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "\n",
    "    #gradients = torch.cat([grad_wf, grad_cn,], 1)\n",
    "    gradients = grad_wf\n",
    "    \n",
    "    gradients = gradients.reshape(gradients.size(0), -1)\n",
    "    gradient_penalty = torch.mean((gradients.norm(2, dim=1) - 1.0/ndim) ** 2)    \n",
    "\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b80cbd4",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1060a8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_WGANO(D, G, epochs, D_optim, G_optim, scheduler=None):\n",
    "    # record the loss information\n",
    "    losses_D = np.zeros(epochs)\n",
    "    losses_G = np.zeros(epochs)\n",
    "    losses_G_val = np.zeros(epochs)\n",
    "    losses_W = np.zeros(epochs)\n",
    "    for i in range(epochs):\n",
    "        loss_D = 0.0\n",
    "        loss_G = 0.0\n",
    "        loss_G_val = 0.0\n",
    "        loss_W = 0.0\n",
    "        for j in range(n_train_tot):\n",
    "            for k in range(n_critic):\n",
    "                D_optim.zero_grad()\n",
    "                # sdat_train is the dataloader for training dataset\n",
    "                (x, log10_PGA, cvs) = sdat_train.get_rand_batch()\n",
    "                x = torch.Tensor(x)\n",
    "                x = F.pad(x, [0, npad]).to(device)\n",
    "\n",
    "                # label shape [batch, 1, 4]\n",
    "                label = np.asarray(cvs)\n",
    "                label = torch.from_numpy(label).permute(1, 2, 0).float().to(device) \n",
    "                #print(\"label shape:{}\".format(label.shape))\n",
    "                \n",
    "                x_syn = G(grf.sample(x.shape[0]).to(device), label)\n",
    "                #print(\"x_syn shape:{}\".format(x_syn.shape))\n",
    "                \n",
    "                # wasserstein regularizaiton\n",
    "                log10_PGA = torch.from_numpy(log10_PGA).unsqueeze(2).float()\n",
    "                log10_PGA = log10_PGA.repeat(1, 1, (ndim+npad)).to(device)\n",
    "                x = torch.cat([x, log10_PGA], dim=1)\n",
    "                \n",
    "                W_loss = -torch.mean(D(x, label)) + torch.mean(D(x_syn, label))\n",
    "                gradient_penalty = calculate_gradient_penalty(D, x, x_syn, label, device)\n",
    "                #gradient_penalty = 0\n",
    "\n",
    "                loss = W_loss + λ_grad * gradient_penalty \n",
    "                loss.backward()\n",
    "                \n",
    "                loss_D += loss.item()\n",
    "                loss_W += W_loss.item()\n",
    "\n",
    "                D_optim.step()\n",
    "            \n",
    "            G_optim.zero_grad()\n",
    "            # train discriminator every n_critic times before updating the generator\n",
    "            (x, _, cvs) = sdat_train.get_rand_batch()\n",
    "            x = torch.Tensor(x).to(device)\n",
    "            \n",
    "            label = np.asarray(cvs)\n",
    "            label = torch.from_numpy(label).permute(1, 2, 0).float().to(device) \n",
    "\n",
    "            x_syn = G(grf.sample(x.shape[0]).to(device), label)\n",
    "\n",
    "            loss = -torch.mean(D(x_syn, label))\n",
    "            loss.backward()\n",
    "            loss_G += loss.item()\n",
    "\n",
    "            G_optim.step()\n",
    "            \n",
    "            # Store validation information\n",
    "            with torch.no_grad():\n",
    "                print(\"epoch:[{} / {}] batch:[{} / {}],loss_G:{:.4f}\".format(i, epochs, j, n_train_tot, loss.item()))   \n",
    "                \n",
    "                # save training loss and validation loss \n",
    "                (x, _, cvs) = sdat_val.get_rand_batch()\n",
    "                x = torch.Tensor(x).to(device)\n",
    "\n",
    "                label = np.asarray(cvs)\n",
    "                label = torch.from_numpy(label).permute(1, 2, 0).float().to(device) \n",
    "\n",
    "                x_syn = G(grf.sample(x.shape[0]).to(device), label)\n",
    "\n",
    "                loss = -torch.mean(D(x_syn, label))\n",
    "                loss_G_val += loss.item()\n",
    "                \n",
    "                if j % 200 == 0:\n",
    "                    # check the label. \n",
    "                    mag = sdat_train.to_real(cvs[0][0], 'magnitude')\n",
    "                    dist = sdat_train.to_real(cvs[1][0], 'rrup')\n",
    "                    vs30 = sdat_train.to_real(cvs[2][0], 'vs30')\n",
    "                    tectonic_value = sdat_train.to_real(cvs[3][0],'tectonic_value')\n",
    "                    if tectonic_value == 0.0:\n",
    "                        tectonic_type = 'Subduction'\n",
    "                    else:\n",
    "                        tectonic_type = 'Shallow crustal'\n",
    "                    fig, ax = plt.subplots(1, 1, figsize=(16,8), tight_layout=True)\n",
    "                    ax.plot(x_syn[0,0,:].squeeze().detach().cpu().numpy())\n",
    "                    ax.set_title('M {} , {} km, $Vs_{{30}}$={}m/s, event= {}'.format(mag[0], dist[0], vs30[0], tectonic_type), fontsize=16)\n",
    "                    plt.savefig('./plots/epoch{}_it{}_GANO'.format(i,j))\n",
    "                    plt.close(fig)\n",
    "\n",
    "                                \n",
    "        losses_D[i] = loss_D / batch_size\n",
    "        losses_G[i] = loss_G / batch_size\n",
    "        losses_G_val[i] = loss_G_val / batch_size\n",
    "        losses_W[i] = loss_W / batch_size\n",
    "        \n",
    "        D_scheduler.step()\n",
    "        G_scheduler.step()\n",
    "        if (i+1) % 10 == 0: #save the model every 10 epochs\n",
    "            torch.save(G.state_dict(), \"./saved_models/G_{}_GANO.pt\".format(i+1))\n",
    "        \n",
    "    return losses_D, losses_G, losses_G_val, losses_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e61fb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder is not exist\n",
    "folder = \"GANO_kik_net_training\"\n",
    "if not os.path.exists(f\"./saved_models/{folder}\"):\n",
    "    os.makedirs(f\"./saved_models/{folder}\")\n",
    "if not os.path.exists(f\"./plots/{folder}\"):\n",
    "    os.makedirs(f\"./plots/{folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f7e374",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timeit.default_timer() # track the time for training\n",
    "losses_D, losses_G, losses_G_val, losses_W = train_WGANO(D, G, epochs, D_optim, G_optim)\n",
    "stop = timeit.default_timer() \n",
    "# print time loss\n",
    "print(stop - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2d0668",
   "metadata": {},
   "source": [
    "### Save the loss information (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c610685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24,4))\n",
    "plt.subplot(1,4,1)\n",
    "plt.plot(losses_D[10:])\n",
    "plt.subplot(1,4,2)\n",
    "plt.plot(losses_G[10:])\n",
    "plt.subplot(1,4,3)\n",
    "plt.plot(losses_G_val[10:])\n",
    "plt.subplot(1,4,4)\n",
    "plt.plot(losses_W[10:])\n",
    "\n",
    "losses_all = pd.DataFrame()\n",
    "\n",
    "losses_all['losses_D'] = losses_D\n",
    "losses_all['losses_G'] = losses_G\n",
    "losses_all['losses_G_val'] = losses_G_val\n",
    "losses_all['losses_W'] = losses_W\n",
    "\n",
    "losses_all.to_csv(\"./losses_GANO.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a834e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30 epochs "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
